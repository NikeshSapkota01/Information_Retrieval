{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Conventry Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape coventry publications page\n",
    "\n",
    "def scrapPapers(start_page = 1, page_limit = 200):\n",
    "\n",
    "    page = start_page\n",
    "    url = f\"https://pureportal.coventry.ac.uk/en/publications/?format=&page={page}\"\n",
    "\n",
    "    papers = []\n",
    "\n",
    "    while page < page_limit:\n",
    "\n",
    "        try:\n",
    "            pageSource = requests.get(url).text\n",
    "            soup = BeautifulSoup(pageSource, \"html.parser\")\n",
    "            paperLists = soup.select(\".list-result-item\")\n",
    "\n",
    "            if len(paperLists) == 0:\n",
    "                break\n",
    "\n",
    "            for paper in paperLists:\n",
    "                paperInfo = {}\n",
    "                paperInfo['link'] = paper.select_one('h3.title a')['href']\n",
    "                paperInfo['title'] = paper.select_one('h3.title a').text\n",
    "\n",
    "                journal = paper.select_one('a', attrs = {'rel' : 'Journal'})\n",
    "                paperInfo['journal'] = journal.text\n",
    "                paperInfo['journalLink'] = journal['href']\n",
    "                cols = ['date', 'volume', 'pages', 'numberofpages', 'type_classification']\n",
    "\n",
    "                for x in cols:\n",
    "                    try:\n",
    "                        paperInfo[x] = paper.select_one(f'span.{x}').text\n",
    "\n",
    "                        if x == 'numberofpages':\n",
    "                            paperInfo[x] = int(paperInfo[x][:-2])\n",
    "                        elif x == 'pages':\n",
    "                            paperInfo[x] = paperInfo[x][3:]\n",
    "                        elif x == 'volume':\n",
    "                            paperInfo[x] = int(paperInfo[x])\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                papers.append(paperInfo)\n",
    "\n",
    "            print(f\"Finished {page} \")\n",
    "            \n",
    "            page += 1\n",
    "            url = f\"https://pureportal.coventry.ac.uk/en/publications/?format=&page={page}\"\n",
    "\n",
    "        except: \n",
    "            break\n",
    "\n",
    "    return papers\n",
    "\n",
    "def getAuthorsAndOtherDocumentInformation(paperInfo):\n",
    "    \n",
    "    source = requests.get(paperInfo['link']).text\n",
    "    paperSoup = BeautifulSoup(source, \"html.parser\")\n",
    "    \n",
    "    if paperSoup.select_one(\"div.doi a\") is not None:\n",
    "        paperInfo['doi'] = paperSoup.select_one(\"div.doi a\")['href']\n",
    "    \n",
    "    persons = paperSoup.select_one(\"p.relations.persons\")\n",
    "    \n",
    "    if persons is not None:\n",
    "        paperInfo['authors'] = list(map(\n",
    "                lambda x : x.strip(), \n",
    "                persons.text.split(','))\n",
    "            )\n",
    "    \n",
    "    paperInfo['tags'] = [span.text for span in \n",
    "            paperSoup.select(\"li.userdefined-keyword\")]\n",
    "\n",
    "    paperInfo['coventryAuthors'] = [a['href'] for a in \n",
    "            persons.select('a', attrs = { 'rel' : 'Person'})]\n",
    "    \n",
    "    abstract = paperSoup.select_one(\".rendering_researchoutput_abstractportal\")\n",
    "\n",
    "    paperInfo['abstract'] = None\n",
    "\n",
    "    if abstract:\n",
    "        paperInfo['abstract'] = abstract.text\n",
    "    \n",
    "    # return paperInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 \n",
      "Finished 2 \n",
      "Finished 3 \n",
      "Finished 4 \n",
      "Finished 5 \n",
      "Finished 6 \n",
      "Finished 7 \n",
      "Finished 8 \n",
      "Finished 9 \n",
      "Finished 10 \n",
      "Finished 11 \n",
      "Finished 12 \n",
      "Finished 13 \n",
      "Finished 14 \n",
      "Finished 15 \n",
      "Finished 16 \n",
      "Finished 17 \n",
      "Finished 18 \n",
      "Finished 19 \n",
      "Finished 20 \n",
      "Finished 21 \n",
      "Finished 22 \n",
      "Finished 23 \n",
      "Finished 24 \n",
      "Finished 25 \n",
      "Finished 26 \n",
      "Finished 27 \n",
      "Finished 28 \n",
      "Finished 29 \n",
      "Finished 30 \n",
      "Finished 31 \n",
      "Finished 32 \n",
      "Finished 33 \n",
      "Finished 34 \n",
      "Finished 35 \n",
      "Finished 36 \n",
      "Finished 37 \n",
      "Finished 38 \n",
      "Finished 39 \n",
      "Finished 40 \n",
      "Finished 41 \n",
      "Finished 42 \n",
      "Finished 43 \n",
      "Finished 44 \n",
      "Finished 45 \n",
      "Finished 46 \n",
      "Finished 47 \n",
      "Finished 48 \n",
      "Finished 49 \n",
      "Finished 50 \n",
      "Finished 51 \n",
      "Finished 52 \n",
      "Finished 53 \n",
      "Finished 54 \n",
      "Finished 55 \n",
      "Finished 56 \n",
      "Finished 57 \n",
      "Finished 58 \n",
      "Finished 59 \n",
      "Finished 60 \n",
      "Finished 61 \n",
      "Finished 62 \n",
      "Finished 63 \n",
      "Finished 64 \n",
      "Finished 65 \n",
      "Finished 66 \n",
      "Finished 67 \n",
      "Finished 68 \n",
      "Finished 69 \n",
      "Finished 70 \n",
      "Finished 71 \n",
      "Finished 72 \n",
      "Finished 73 \n",
      "Finished 74 \n",
      "Finished 75 \n",
      "Finished 76 \n",
      "Finished 77 \n",
      "Finished 78 \n",
      "Finished 79 \n",
      "Finished 80 \n",
      "Finished 81 \n",
      "Finished 82 \n",
      "Finished 83 \n",
      "Finished 84 \n",
      "Finished 85 \n",
      "Finished 86 \n",
      "Finished 87 \n",
      "Finished 88 \n",
      "Finished 89 \n",
      "Finished 90 \n",
      "Finished 91 \n",
      "Finished 92 \n",
      "Finished 93 \n",
      "Finished 94 \n",
      "Finished 95 \n",
      "Finished 96 \n",
      "Finished 97 \n",
      "Finished 98 \n",
      "Finished 99 \n",
      "Finished 100 \n",
      "Finished 101 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5057"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = scrapPapers(start_page = 1)\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"papers.json\", \"w\") as f:\n",
    "    f.write(json.dumps(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': 'https://pureportal.coventry.ac.uk/en/publications/association-of-minimally-processed-and-ultra-processed-food-daily',\n",
       " 'title': 'Association of minimally processed and ultra-processed food daily consumption with obesity in overweight adults:  a cross-sectional study',\n",
       " 'journal': 'Association of minimally processed and ultra-processed food daily consumption with obesity in overweight adults:  a cross-sectional study',\n",
       " 'journalLink': 'https://pureportal.coventry.ac.uk/en/publications/association-of-minimally-processed-and-ultra-processed-food-daily',\n",
       " 'date': '15 Feb 2023',\n",
       " 'volume': '(In-Press)',\n",
       " 'numberofpages': 30,\n",
       " 'type_classification': 'Article',\n",
       " 'doi': 'https://doi.org/10.20960/nh.04270',\n",
       " 'authors': ['Leonardo V Silva',\n",
       "  'Pedro Pugliesi Abdalla',\n",
       "  'Lucimere Bohn',\n",
       "  'Rafael Gavassa de Araújo',\n",
       "  'Daniel de Freitas Batalhão',\n",
       "  'Ana Cláudia Rossini Venturini',\n",
       "  'Anderson Dos Santos Carvalho',\n",
       "  'Michael Duncan',\n",
       "  'Jorge Mota',\n",
       "  'Dalmo Roberto Lopes Machado'],\n",
       " 'tags': ['Non-communicable diseases', 'Nutrition', 'Nutritional status'],\n",
       " 'coventryAuthors': ['https://pureportal.coventry.ac.uk/en/persons/michael-duncan'],\n",
       " 'abstract': \"food type represents higher odds of having obesity (OB), especially in overweight (OW) subjects. Minimally and ultra-processed foods can be associated with the odds of having OB in OW subjects. to investigate the association of minimally and ultra-processed food consumption with OB in OW adults. we included 15,024 participants (9,618 OW [25.0-29.9 kg/m2], 5,406 OB [≥ 30 kg/m2]) with ages ranging from 18 to 59 years from the 2019 baseline survey of the Surveillance of Risk Factors and Protection for Chronic Diseases by Telephone Survey (VIGITEL, Brazil). Minimally and ultra-processed food daily consumption scores and confounding variables (age, sex, scholarly, physical activity, hypertension, and diabetes) were measured. Binary logistic regression analyzes the association of minimally and ultra-processed food consumption scores with OB (odds ratio [OR]). minimally processed food consumption score quartiles (1st = 1[food-score/day]; 2nd = 6[food-score/day]; 3rd = 7[food-score/day]; 4th = 8[food-score/day]) presented higher values compared to ultra-processed food (1st = 1[food-score/day]; 2nd = 1[food-score/day]; 3rd = 2[food-score/day]; 4th = 4[food-score/day]). For each score of minimally processed food consumed, there was a -5.9 % odds of OB. Thus, the higher quartile (4th) of minimally processed food consumption score represents less odds of OB (OR: -47.2 %; p < 0.001). Each ultra-processed food score consumed presented odds of 3.7 % of OB. Therefore, higher consumption of ultra-processed food (4th quartile) shows higher odds of OB (OR: +14.8 %; p < 0.001). All associations remained significatively even after being adjusted by the confounders. the consumption scores of minimally processed and ultra-processed foods presented a magnitude capable of impacting OW adults' odds of OB, even when controlled by sociodemographic factors, physical activity, hypertension, and diabetes.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAuthorsAndOtherDocumentInformation(papers[0])\n",
    "papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, subprocess, time\n",
    " \n",
    "num_threads = 20\n",
    "lock = threading.Lock()\n",
    "last = time.time()\n",
    "\n",
    "def scrapePapersParallel(papers, start, end):\n",
    "    '''\n",
    "        in the range [start, end)\n",
    "    '''\n",
    "\n",
    "    last = time.time()\n",
    "\n",
    "    for i in range(start, end):\n",
    "        try:\n",
    "            getAuthorsAndOtherDocumentInformation(papers[i])\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Finished {i}\")\n",
    "        except:\n",
    "            print(f\"Failed Index {i}\")\n",
    "\n",
    "        try:\n",
    "            now = time.time()\n",
    "\n",
    "            # write to file here synchronize against threads\n",
    "            if threading.current_thread().name == \"Thread-0\":\n",
    "                if (now - last) >= 60:\n",
    "                    lock.acquire()\n",
    "                    last = now\n",
    "\n",
    "                    with open(\"./papers1.json\", \"w\") as f:\n",
    "                        f.write(json.dumps(papers))\n",
    "                    \n",
    "                    print(f\"Wrote to file\")\n",
    "                    lock.release()\n",
    "        except:\n",
    "            print(\"Error writing to file\")\n",
    "            \n",
    "    print(f\"Finished thread {threading.current_thread().name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 252, 504, 756, 1008, 1260, 1512, 1764, 2016, 2268, 2520, 2772, 3024, 3276, 3528, 3780, 4032, 4284, 4536, 4788, 5057]\n",
      "Finished 0\n",
      "Finished 4800\n",
      "Finished 4300\n",
      "Finished 3800\n",
      "Wrote to file\n",
      "Finished 3300\n",
      "Finished 2800\n",
      "Finished 2300\n",
      "Finished 1800\n",
      "Finished 1300\n",
      "Finished 300\n",
      "Finished 800\n",
      "Wrote to file\n",
      "Finished 4600\n",
      "Finished 4100\n",
      "Finished 3600\n",
      "Finished 3100\n",
      "Finished 2600\n",
      "Wrote to file\n",
      "Finished 2100\n",
      "Finished 1600\n",
      "Finished 1100\n",
      "Finished 600\n",
      "Finished 100\n",
      "Finished 4900\n",
      "Wrote to file\n",
      "Finished 3900\n",
      "Finished 4400\n",
      "Finished 3400\n",
      "Finished 2400\n",
      "Finished 2900\n",
      "Finished 1900\n",
      "Finished 1400\n",
      "Finished 400\n",
      "Finished 900\n",
      "Wrote to file\n",
      "Finished 4700\n",
      "Finished 4200\n",
      "Finished 3700\n",
      "Finished 3200\n",
      "Wrote to file\n",
      "Finished 2700\n",
      "Finished 2200\n",
      "Finished 1700\n",
      "Finished 1200\n",
      "Finished 700\n",
      "Finished 200\n",
      "Wrote to file\n",
      "Finished 4000\n",
      "Finished 5000\n",
      "Finished 4500\n",
      "Finished 3500\n",
      "Finished 3000\n",
      "Finished 2500\n",
      "Wrote to file\n",
      "Finished 2000\n",
      "Finished 1500\n",
      "Finished thread Thread-15\n",
      "Finished 500\n",
      "Finished thread Thread-16\n",
      "Finished thread Thread-5\n",
      "Finished thread Thread-14\n",
      "Finished 1000\n",
      "Finished thread Thread-1\n",
      "Finished thread Thread-0\n",
      "Finished thread Thread-7\n",
      "Finished thread Thread-12\n",
      "Finished thread Thread-11\n",
      "Finished thread Thread-9\n",
      "Finished thread Thread-13\n",
      "Finished thread Thread-18\n",
      "Finished thread Thread-10\n",
      "Finished thread Thread-8\n",
      "Finished thread Thread-6\n",
      "Finished thread Thread-4\n",
      "Finished thread Thread-2\n",
      "Finished thread Thread-3\n",
      "Finished thread Thread-17\n",
      "Finished thread Thread-19\n"
     ]
    }
   ],
   "source": [
    "blockSizes = len(papers) // num_threads\n",
    "\n",
    "startInds = [i * blockSizes for i in range(0, num_threads)]\n",
    "startInds.append(len(papers))\n",
    "print(startInds)\n",
    "\n",
    "threads = []\n",
    "\n",
    "for ind in range(num_threads):\n",
    "    new = threading.Thread(\n",
    "        target=scrapePapersParallel,\n",
    "        args=(papers, startInds[ind], startInds[ind + 1]),\n",
    "        name=f\"Thread-{ind}\"\n",
    "    )\n",
    "\n",
    "    threads.append(new)\n",
    "    new.start()\n",
    "\n",
    "over_threads = iter(threads)\n",
    "curr_th = next(over_threads)\n",
    "\n",
    "while True:\n",
    "    curr_th.join()\n",
    "    if curr_th.is_alive():\n",
    "        continue\n",
    "    try:\n",
    "        curr_th = next(over_threads)\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"papers.json\", \"w\") as f:\n",
    "    f.write(json.dumps(papers))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'link': 'https://pureportal.coventry.ac.uk/en/publications/from-anglophone-problem-to-anglophone-conflict-in-cameroon-assess',\n",
       " 'title': 'From ‘Anglophone Problem’ to ‘Anglophone Conflict’ in Cameroon: Assessing Prospects for Peace',\n",
       " 'journal': 'From ‘Anglophone Problem’ to ‘Anglophone Conflict’ in Cameroon: Assessing Prospects for Peace',\n",
       " 'journalLink': 'https://pureportal.coventry.ac.uk/en/publications/from-anglophone-problem-to-anglophone-conflict-in-cameroon-assess',\n",
       " 'date': 'Apr 2023',\n",
       " 'volume': 58,\n",
       " 'pages': '89-105',\n",
       " 'numberofpages': 17,\n",
       " 'type_classification': 'Article',\n",
       " 'doi': 'https://doi.org/10.1177/00020397231155244',\n",
       " 'authors': ['Maurice Beseng', 'Gordon Crawford', 'Nancy Annan'],\n",
       " 'tags': ['Cameroon',\n",
       "  'Anglophone problem',\n",
       "  'Anglophone conflict',\n",
       "  'identity',\n",
       "  'conflict resolution'],\n",
       " 'coventryAuthors': ['https://pureportal.coventry.ac.uk/en/persons/gordon-crawford',\n",
       "  'https://pureportal.coventry.ac.uk/en/persons/nancy-annan-2'],\n",
       " 'abstract': 'Since 2017, an armed conflict has been raging in the English-speaking regions of Cameroon between separatist forces and the Cameroonian military. This review analyses the historical origins and root causes of the conflict; the trigger mechanism of rising protests and state repression in 2016; the emergence and evolution of the armed conflict over the past 5 years; its impact on civilians; and hopes for peace. However, there is currently little prospect for conflict resolution as the Cameroon government appears intent on ignoring limited international pressure, maintaining the charade that the ‘security crisis’ is over and reconstruction is underway, while continuing its counter-insurgency strategy to militarily defeat the armed separatist groups. We note that, while the desire for peace is profound, the political status quo is no longer tolerable nor acceptable, with conflict resolution dependent on political changes that provide, at a minimum, the Anglophone regions with greater autonomy and protection of their particular identity and institutions.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"papers.json\", \"r\") as f:\n",
    "   papers = json.loads(f.read())\n",
    "\n",
    "papers[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "papersDf = pd.read_json(\"./papers.json\")\n",
    "# papersDf\n",
    "\n",
    "# export as csv\n",
    "papersDf.to_csv(\"./papers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Author Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProfileURLorNone(url):\n",
    "\n",
    "    if \"no-content\" in url:\n",
    "        return None\n",
    "    \n",
    "    pattern = r\"^(\\/[^?]+)\"\n",
    "    path_match = re.match(pattern, url)\n",
    "    path = None\n",
    "\n",
    "    if path_match:\n",
    "        path = path_match.group(1)\n",
    "\n",
    "    return path    \n",
    "\n",
    "def scrapeAuthors(start_page = 1, page_limit = 200):\n",
    "\n",
    "    page = start_page\n",
    "    url = f\"https://pureportal.coventry.ac.uk/en/persons/?format=&page={page}\"\n",
    "\n",
    "    authors = []\n",
    "\n",
    "    while page < page_limit:\n",
    "\n",
    "        try:\n",
    "            pageSource = requests.get(url).text\n",
    "\n",
    "            soup = BeautifulSoup(pageSource, \"html.parser\")\n",
    "            \n",
    "            authorList = soup.select(\"li.grid-result-item div.result-container\")\n",
    "\n",
    "            if len(authorList) == 0:\n",
    "                break\n",
    "\n",
    "            for author in authorList:\n",
    "\n",
    "                try:\n",
    "                    authorInfo = {}\n",
    "\n",
    "                    authorInfo['picUrl'] = getProfileURLorNone(\n",
    "                            author.select_one(\"img\")['src']\n",
    "                    )\n",
    "\n",
    "                    if authorInfo['picUrl'] is not None:\n",
    "                        authorInfo['picUrl'] = 'https://pureportal.coventry.ac.uk/' + authorInfo['picUrl'] \n",
    "\n",
    "                    name = author.select_one(\"a\", attrs = { 'rel' : 'Person'})\n",
    "\n",
    "                    authorInfo['name'] = name.text\n",
    "                    authorInfo['profileLink'] = name['href']\n",
    "\n",
    "                    dept = author.select_one(\".relations.organisations a\", \n",
    "                            attrs = { 'rel' : 'Organisation'})\n",
    "\n",
    "                    authorInfo['department'] = dept.text\n",
    "                    authorInfo['deptLink'] = dept['href']\n",
    "                    \n",
    "                    authors.append(authorInfo)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            print(f\"Finished {page} \")\n",
    "            \n",
    "            page += 1\n",
    "            url = f\"https://pureportal.coventry.ac.uk/en/persons/?format=&page={page}\"\n",
    "        except: \n",
    "            break\n",
    "\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 \n",
      "Finished 2 \n",
      "Finished 3 \n",
      "Finished 4 \n",
      "Finished 5 \n",
      "Finished 6 \n",
      "Finished 7 \n",
      "Finished 8 \n",
      "Finished 9 \n",
      "Finished 10 \n",
      "Finished 11 \n",
      "Finished 12 \n",
      "Finished 13 \n",
      "Finished 14 \n",
      "Finished 15 \n",
      "Finished 16 \n",
      "Finished 17 \n",
      "Finished 18 \n",
      "Finished 19 \n",
      "Finished 20 \n",
      "Finished 21 \n",
      "Finished 22 \n",
      "Finished 23 \n",
      "Finished 24 \n",
      "Finished 25 \n",
      "Finished 26 \n",
      "Finished 27 \n",
      "Finished 28 \n",
      "Finished 29 \n",
      "Finished 30 \n",
      "Finished 31 \n",
      "Finished 32 \n",
      "Finished 33 \n",
      "Finished 34 \n",
      "Finished 35 \n",
      "Finished 36 \n",
      "Finished 37 \n",
      "Finished 38 \n",
      "Finished 39 \n",
      "Finished 40 \n",
      "Finished 41 \n",
      "Finished 42 \n",
      "Finished 43 \n",
      "Finished 44 \n",
      "Finished 45 \n",
      "Finished 46 \n",
      "Scraped 2026 authors\n"
     ]
    }
   ],
   "source": [
    "authors = scrapeAuthors()\n",
    "\n",
    "print(f\"Scraped {len(authors)} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"authors.json\", \"w\") as f:\n",
    "    f.write(json.dumps(authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert into database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorsDf = pd.read_json(\"./authors.json\")\n",
    "# papersDf\n",
    "\n",
    "# export as csv\n",
    "authorsDf.to_csv(\"./authors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
